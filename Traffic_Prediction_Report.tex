\documentclass[12pt, a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}

% Code styling
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    breaklines=true,
    showstringspaces=false,
    tabsize=4,
    backgroundcolor=\color{lightgray!20}
}

\title{\textbf{Traffic Volume Prediction System\\Metro Interstate Highway}}
\author{Data Science Project}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This report presents an advanced traffic volume prediction system for the Metro Interstate highway using machine learning techniques. The system employs three different models—Linear Regression, Random Forest, and XGBoost—with lag features to capture temporal patterns. The best performing model achieves an $R^2$ score of 0.887 with 88.73\% accuracy. The system includes a web-based interface for real-time traffic predictions based on weather and temporal features.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}

\subsection{Problem Statement}
Accurate traffic volume prediction is crucial for:
\begin{itemize}
    \item Urban traffic management and congestion mitigation
    \item Infrastructure planning and resource allocation
    \item Emergency response coordination
    \item Intelligent transportation systems (ITS)
\end{itemize}

The task is to predict hourly traffic volume on a metro interstate highway using historical traffic data combined with weather and temporal features.

\subsection{Dataset Overview}
\begin{table}[H]
\centering
\caption{Dataset Characteristics}
\begin{tabular}{|l|c|}
\hline
\textbf{Attribute} & \textbf{Value} \\
\hline
Total Records & 48,204 hourly observations \\
Date Range & Oct 2, 2012 - Sep 30, 2018 \\
Target Variable & Traffic Volume (vehicles/hour) \\
Range & 0 - 7,280 vehicles/hour \\
Mean & 3,259.82 vehicles/hour \\
Std Dev & 1,986.86 vehicles/hour \\
\hline
\end{tabular}
\end{table}

\section{Data Description}

\subsection{Features}
The dataset contains the following features:

\begin{table}[H]
\centering
\caption{Feature Descriptions}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Feature} & \textbf{Type} & \textbf{Description} \\
\hline
date\_time & Datetime & Timestamp of measurement \\
temp & Continuous & Temperature in Kelvin (240-320 K) \\
rain\_1h & Continuous & Rain in last hour (mm) \\
snow\_1h & Continuous & Snow in last hour (mm) \\
clouds\_all & Continuous & Cloud coverage (0-100\%) \\
weather\_main & Categorical & Primary weather condition \\
holiday & Categorical & Holiday type (if applicable) \\
traffic\_volume & Target & Hourly vehicle count \\
\hline
\end{tabular}
\end{table}

\subsection{Target Variable}
The target variable is \textit{traffic\_volume}, representing the number of vehicles passing through the highway during each one-hour period.

\begin{equation}
y = \text{traffic\_volume} \in [0, 7280]
\end{equation}

\section{Feature Engineering}

\subsection{Time-Based Features}
From the datetime column, we extract:

\begin{align}
\text{hour} &\in \{0, 1, 2, \ldots, 23\} \\
\text{day} &\in \{1, 2, \ldots, 31\} \\
\text{month} &\in \{1, 2, \ldots, 12\} \\
\text{weekday} &\in \{0, 1, 2, \ldots, 6\} \quad \text{(Monday=0, Sunday=6)}
\end{align}

\subsection{Lag Features}
Lag features capture temporal dependencies in traffic patterns:

\begin{align}
\text{lag\_1h} &= y_{t-1} \quad \text{(traffic volume 1 hour ago)} \\
\text{lag\_6h} &= y_{t-6} \quad \text{(traffic volume 6 hours ago)} \\
\text{lag\_24h} &= y_{t-24} \quad \text{(traffic volume 24 hours ago)}
\end{align}

These features are particularly valuable for capturing:
\begin{itemize}
    \item Short-term traffic momentum (lag\_1h)
    \item Morning/evening commute patterns (lag\_6h)
    \item Day-of-week effects (lag\_24h)
\end{itemize}

\subsection{Categorical Encoding}
Categorical variables are one-hot encoded:

\begin{equation}
\text{weather\_main} \in \{\text{Clear, Clouds, Rain, Snow, Mist, \ldots}\}
\end{equation}

\begin{equation}
\text{holiday} \in \{\text{None, Christmas, Independence Day, \ldots}\}
\end{equation}

\section{Data Preprocessing}

\subsection{Cleaning Steps}
\begin{enumerate}
    \item Remove rows with missing traffic volume
    \item Filter outliers: $0 \leq \text{traffic\_volume} \leq 10,000$
    \item Clip extreme weather values using quantile clipping (0.1\% - 99.9\%)
    \item Drop redundant columns (weather\_description)
\end{enumerate}

\subsection{Train-Test Split}
A time-based split is used to prevent data leakage:

\begin{equation}
\text{cutoff\_date} = \text{2018-01-01}
\end{equation}

\begin{align}
\text{Training Set} &: \text{dates} < \text{2018-01-01} \\
\text{Test Set} &: \text{dates} \geq \text{2018-01-01}
\end{align}

This temporal split ensures realistic evaluation where the model predicts future traffic based on historical data.

\section{Machine Learning Models}

\subsection{Model 1: Linear Regression}

\subsubsection{Theory}
Linear Regression models the relationship between features and target as:

\begin{equation}
\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_n x_n
\end{equation}

where $\beta_i$ are coefficients learned by minimizing Mean Squared Error (MSE):

\begin{equation}
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\end{equation}

\subsubsection{Implementation}
\begin{lstlisting}
from sklearn.linear_model import LinearRegression

lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred = lr.predict(X_test)
\end{lstlisting}

\subsection{Model 2: Random Forest}

\subsubsection{Theory}
Random Forest is an ensemble method that builds multiple decision trees and averages their predictions:

\begin{equation}
\hat{y}_{\text{RF}} = \frac{1}{B} \sum_{b=1}^{B} T_b(x)
\end{equation}

where $B$ is the number of trees and $T_b$ is the $b$-th tree.

\subsubsection{Hyperparameters}
\begin{table}[H]
\centering
\caption{Random Forest Configuration}
\begin{tabular}{|l|c|}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
n\_estimators & 400 \\
max\_depth & None (unlimited) \\
random\_state & 42 \\
n\_jobs & -1 (parallel processing) \\
\hline
\end{tabular}
\end{table}

\subsubsection{Implementation}
\begin{lstlisting}
from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(
    n_estimators=400,
    max_depth=None,
    random_state=42,
    n_jobs=-1
)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
\end{lstlisting}

\subsection{Model 3: XGBoost}

\subsubsection{Theory}
XGBoost (Extreme Gradient Boosting) is a gradient boosting framework that iteratively builds trees:

\begin{equation}
\hat{y}^{(t)} = \hat{y}^{(t-1)} + f_t(x)
\end{equation}

where $f_t$ is a new tree fitted to residuals of previous iteration.

The objective function minimized is:

\begin{equation}
\text{Obj} = \sum_{i=1}^{n} L(y_i, \hat{y}_i) + \sum_{t=1}^{T} \Omega(f_t)
\end{equation}

where $L$ is the loss function and $\Omega$ is the regularization term.

\subsubsection{Hyperparameters}
\begin{table}[H]
\centering
\caption{XGBoost Configuration}
\begin{tabular}{|l|c|}
\hline
\textbf{Parameter} & \textbf{Value} \\
\hline
n\_estimators & 500 \\
learning\_rate & 0.05 \\
max\_depth & 8 \\
subsample & 0.8 \\
colsample\_bytree & 0.8 \\
tree\_method & hist \\
\hline
\end{tabular}
\end{table}

\subsubsection{Implementation}
\begin{lstlisting}
from xgboost import XGBRegressor

xgb = XGBRegressor(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=8,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    n_jobs=-1,
    tree_method="hist"
)
xgb.fit(X_train, y_train)
y_pred = xgb.predict(X_test)
\end{lstlisting}

\section{Model Evaluation}

\subsection{Evaluation Metrics}

\subsubsection{Mean Absolute Error (MAE)}
Measures average absolute prediction error:

\begin{equation}
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
\end{equation}

\subsubsection{Root Mean Squared Error (RMSE)}
Penalizes larger errors more heavily:

\begin{equation}
\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
\end{equation}

\subsubsection{$R^2$ Score (Coefficient of Determination)}
Measures proportion of variance explained:

\begin{equation}
R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
\end{equation}

where $\bar{y}$ is the mean of observed values.

Range: $R^2 \in (-\infty, 1]$, with 1 being perfect prediction.

\subsubsection{Accuracy}
For regression, we define accuracy as:

\begin{equation}
\text{Accuracy} = R^2 \times 100\% \quad \text{(clamped to [0, 100])}
\end{equation}

\subsection{Results Comparison}

\begin{table}[H]
\centering
\caption{Model Performance Metrics}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Model} & \textbf{MAE} & \textbf{RMSE} & \textbf{$R^2$} & \textbf{Accuracy} \\
\hline
\textbf{Linear Regression} & 77.77 & 95.46 & 0.887 & 88.73\% \\
Random Forest & 114.22 & 132.77 & 0.782 & 78.21\% \\
XGBoost & 147.52 & 159.81 & 0.684 & 68.42\% \\
\hline
\end{tabular}
\end{table}

\subsection{Best Model Selection}
Linear Regression is selected as the best model based on lowest RMSE:

\begin{equation}
\text{Best Model} = \arg\min_{\text{model}} \text{RMSE}_{\text{model}}
\end{equation}

\textbf{Selected Model:} Linear Regression with RMSE = 95.46 vehicles/hour

\section{Mathematical Formulation}

\subsection{Final Prediction Formula}
The Linear Regression model for traffic prediction:

\begin{equation}
\widehat{\text{Traffic}} = \beta_0 + \sum_{i=1}^{27} \beta_i x_i
\end{equation}

where $x_i$ includes:
\begin{itemize}
    \item Lag features: $\text{lag\_1h}, \text{lag\_6h}, \text{lag\_24h}$
    \item Time features: hour, day, month, weekday
    \item Weather features: temp, rain\_1h, snow\_1h, clouds\_all
    \item Encoded categorical: weather\_main, holiday
\end{itemize}

\subsection{Error Analysis}
Mean prediction error:

\begin{equation}
\epsilon = \frac{1}{n_{\text{test}}} \sum_{i=1}^{n_{\text{test}}} (\hat{y}_i - y_i) \approx \pm 95.46 \text{ vehicles/hour}
\end{equation}

This means the model typically predicts traffic within ±95 vehicles per hour on average.

\section{System Architecture}

\subsection{Data Pipeline}
\begin{lstlisting}
Raw Data
    ↓
Load & Parse (date_time conversion)
    ↓
Engineer Time Features (hour, day, month, weekday)
    ↓
Data Cleaning (remove NaN, clip outliers)
    ↓
Add Lag Features (lag_1h, lag_6h, lag_24h)
    ↓
One-Hot Encoding (weather_main, holiday)
    ↓
Train-Test Split (temporal cutoff: 2018-01-01)
    ↓
Feature Scaling (implicit in Linear Regression)
    ↓
Model Training
\end{lstlisting}

\subsection{Web Application}
A Streamlit web application provides:
\begin{itemize}
    \item Interactive prediction interface
    \item Real-time feature importance visualization
    \item Actual vs Predicted plots
    \item Parameter adjustment controls
\end{itemize}

\section{Results and Insights}

\subsection{Key Findings}
\begin{enumerate}
    \item \textbf{Linear Regression Superiority:} Linear models outperform tree-based models on this dataset, suggesting linear relationships between features and traffic volume.
    
    \item \textbf{Lag Features Importance:} Historical traffic values (especially lag\_1h) are strong predictors, indicating traffic momentum.
    
    \item \textbf{Temporal Patterns:} Hour-of-day and weekday features capture commute patterns effectively.
    
    \item \textbf{Weather Impact:} Temperature and cloud coverage show moderate correlation with traffic volume.
\end{enumerate}

\subsection{Model Predictions}
Example predictions with 95\% confidence intervals:

\begin{equation}
\hat{y} = 3500 \pm 95.46 \text{ vehicles/hour}
\end{equation}

\section{Conclusion}

The developed traffic prediction system achieves:
\begin{itemize}
    \item 88.73\% accuracy (R² = 0.887) using Linear Regression
    \item Robust predictions with MAE of 77.77 vehicles/hour
    \item Effective incorporation of temporal and weather features
    \item Practical web interface for real-world deployment
\end{itemize}

The system can be used for:
\begin{itemize}
    \item Traffic management optimization
    \item Congestion prediction
    \item Resource allocation planning
    \item Intelligent transportation system integration
\end{itemize}

\section{Future Work}

\begin{enumerate}
    \item \textbf{Ensemble Methods:} Combine predictions from all three models
    \item \textbf{LSTM Networks:} Implement deep learning for sequence modeling
    \item \textbf{Exogenous Variables:} Incorporate holidays, events, accidents
    \item \textbf{Hyperparameter Optimization:} Use GridSearchCV or Bayesian optimization
    \item \textbf{Real-time Updates:} Implement online learning for model updates
\end{enumerate}

\section{References}

\begin{thebibliography}{99}

\bibitem{sklearn}
Pedregosa, F., et al. (2011). Scikit-learn: Machine learning in Python. \textit{Journal of Machine Learning Research}, 12, 2825-2830.

\bibitem{xgboost}
Chen, T., \& Guestrin, C. (2016). XGBoost: A scalable tree boosting system. In \textit{Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining} (pp. 785-794).

\bibitem{streamlit}
Streamlit Documentation. Retrieved from \url{https://docs.streamlit.io/}

\end{thebibliography}

\appendix

\section{Code Implementation}

\subsection{Model Training}
\begin{lstlisting}
# src/train.py
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, r2_score

def evaluate(y_true, y_pred):
    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true, y_pred)
    accuracy = r2 * 100 if r2 > 0 else 0
    return {"mae": float(mae), 
            "rmse": float(rmse), 
            "r2_score": float(r2), 
            "accuracy": float(accuracy)}
\end{lstlisting}

\subsection{Data Preparation}
\begin{lstlisting}
# src/data_prep.py
def prepare_dataset(csv_path: str, cutoff="2018-01-01"):
    df = load_data(csv_path)
    df = engineer_time_features(df)
    df = clean_and_prepare(df)
    df = add_lag_features(df, [1, 6, 24])
    df = one_hot_encode(df, cols=("weather_main", "holiday"))
    train, test = train_test_time_split(df, cutoff=cutoff)
    X_train, y_train = feature_target_split(train)
    X_test, y_test = feature_target_split(test)
    return df, train, test, X_train, y_train, X_test, y_test
\end{lstlisting}

\end{document}
